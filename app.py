import streamlit as st
import time
from google import genai
import pypdf
import json
import re
import sys
import io
import traceback
from datetime import datetime

# ==========================================
# ğŸŒŸ å‘é‡æª¢ç´¢ç›¸é—œå¥—ä»¶
# ==========================================
try:
    from sentence_transformers import SentenceTransformer
    import numpy as np
    VECTOR_AVAILABLE = True
except ImportError:
    VECTOR_AVAILABLE = False
    st.warning("âš ï¸ æœªå®‰è£ sentence-transformersï¼Œå°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆæª¢ç´¢ã€‚è«‹åŸ·è¡Œï¼špip install sentence-transformers")

# --- 1. ç¶²é åŸºç¤è¨­å®š ---
st.set_page_config(
    page_title="AI Agent è«–æ–‡åŠ©æ‰‹", 
    layout="wide", 
    page_icon="ğŸ¤–",
    initial_sidebar_state="expanded"
)

# è‡ªå®šç¾© CSS æ¨£å¼ - ç°¡ç´„é»‘ç°ç™½é¢¨æ ¼
st.markdown("""
<style>
    /* ä¸»è¦å®¹å™¨ */
    .main {
        padding: 0rem 1rem;
    }
    
    /* èŠå¤©è¨Šæ¯å®¹å™¨ */
    .stChatMessage {
        padding: 1rem;
        margin: 0.5rem 0;
    }
    
    /* æ¨™é¡Œæ¨£å¼ */
    h1 {
        font-size: 1.5rem;
        font-weight: 500;
        margin-bottom: 1rem;
    }
    
    /* ç§»é™¤æ‰€æœ‰æŒ‰éˆ•çš„åˆºçœ¼é¡è‰² */
    .stButton > button {
        background-color: transparent;
        border: 1px solid rgba(250, 250, 250, 0.2);
        color: inherit;
    }
    
    .stButton > button:hover {
        border-color: rgba(250, 250, 250, 0.4);
        background-color: rgba(250, 250, 250, 0.05);
    }
</style>
""", unsafe_allow_html=True)

# --- è¼”åŠ©å‡½å¼ï¼šæ¸…æ´— JSON ---
def clean_json_string(text):
    if not text: return None
    text = re.sub(r'```json\s*', '', text)
    text = re.sub(r'```\s*', '', text)
    return text.strip()

# ==========================================
# ğŸŒŸ LangChain é¢¨æ ¼ï¼šå‘é‡æª¢ç´¢è¨˜æ†¶ç®¡ç†å™¨
# ==========================================
class VectorMemoryManager:
    """
    å®Œå…¨ç¬¦åˆåœ–ç‰‡æµç¨‹çš„å‘é‡æª¢ç´¢ç³»çµ±
    æµç¨‹ï¼šText â†’ Text Splitter â†’ Embedding â†’ VectorStore â†’ Similarity Search
    """
    def __init__(self):
        self.chunks = []  # å„²å­˜æ–‡æœ¬å¡Š
        self.embeddings = []  # å„²å­˜å‘é‡
        self.embedding_model = None
        
        # åˆå§‹åŒ– Embedding æ¨¡å‹ï¼ˆæ­¥é©Ÿ 5ï¼‰
        if VECTOR_AVAILABLE:
            try:
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            except Exception as e:
                st.error(f"âŒ Embedding æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
                self.embedding_model = None
    
    def load_pdf(self, reader):
        """
        æ­¥é©Ÿ 1-4: Local Documents â†’ Unstructured Loader â†’ Text â†’ Text Splitter
        æ­¥é©Ÿ 5-6: Text Chunks â†’ Embedding â†’ VectorStore
        """
        # æ­¥é©Ÿ 1-2: è¼‰å…¥æ–‡ä»¶
        full_text = ""
        for page in reader.pages:
            text = page.extract_text()
            if text: 
                full_text += text + "\n"
        
        # æ­¥é©Ÿ 3-4: Text Splitterï¼ˆåˆ†å‰²æˆ chunksï¼‰
        chunk_size = 1000
        chunk_overlap = 200  # é‡ç–Šéƒ¨åˆ†ï¼Œæé«˜æª¢ç´¢é€£è²«æ€§
        
        self.chunks = []
        for i in range(0, len(full_text), chunk_size - chunk_overlap):
            chunk = full_text[i:i + chunk_size]
            if chunk.strip():
                self.chunks.append(chunk)
        
        # æ­¥é©Ÿ 5-6: Embeddingï¼ˆå°‡æ–‡æœ¬è½‰æ›æˆå‘é‡ï¼‰
        if self.embedding_model and self.chunks:
            try:
                self.embeddings = self.embedding_model.encode(
                    self.chunks, 
                    show_progress_bar=False,
                    convert_to_numpy=True
                )
            except Exception as e:
                st.error(f"âŒ å‘é‡åŒ–å¤±æ•—ï¼š{e}")
                self.embeddings = []
        
        return len(self.chunks)
    
    def retrieve(self, query, top_k=3):
        """
        æ­¥é©Ÿ 8-11: Query â†’ Embedding â†’ Query Vector â†’ Vector Similarity â†’ Related Text Chunks
        """
        if not self.chunks:
            return "(ç›®å‰å°šç„¡æª¢ç´¢å…§å®¹)"
        
        # å¦‚æœæ²’æœ‰å‘é‡æ¨¡å‹ï¼Œä½¿ç”¨ç°¡å–®å­—ä¸²åŒ¹é…ï¼ˆé™ç´šæ–¹æ¡ˆï¼‰
        if not self.embedding_model or len(self.embeddings) == 0:
            return self._fallback_retrieve(query, top_k)
        
        try:
            # æ­¥é©Ÿ 8-9: Query â†’ Embeddingï¼ˆå°‡å•é¡Œè½‰æ›æˆå‘é‡ï¼‰
            query_vector = self.embedding_model.encode(
                [query], 
                convert_to_numpy=True
            )[0]
            
            # æ­¥é©Ÿ 10: Vector Similarityï¼ˆè¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰
            similarities = []
            for i, chunk_vector in enumerate(self.embeddings):
                # é¤˜å¼¦ç›¸ä¼¼åº¦è¨ˆç®—
                similarity = np.dot(query_vector, chunk_vector) / (
                    np.linalg.norm(query_vector) * np.linalg.norm(chunk_vector)
                )
                similarities.append((i, similarity))
            
            # æ­¥é©Ÿ 11: æ’åºä¸¦å–å‡ºæœ€ç›¸é—œçš„ chunks
            similarities.sort(key=lambda x: x[1], reverse=True)
            top_indices = [idx for idx, _ in similarities[:top_k]]
            
            # æ­¥é©Ÿ 12: Related Text Chunks
            related_chunks = [self.chunks[i] for i in top_indices]
            retrieved_text = "\n\n---\n\n".join(related_chunks)
            
            return retrieved_text
        
        except Exception as e:
            st.error(f"âŒ å‘é‡æª¢ç´¢å¤±æ•—ï¼š{e}")
            return self._fallback_retrieve(query, top_k)
    
    def _fallback_retrieve(self, query, top_k=3):
        """é™ç´šæ–¹æ¡ˆï¼šç°¡å–®å­—ä¸²åŒ¹é…"""
        relevant_chunks = []
        for chunk in self.chunks:
            if query.lower() in chunk.lower():
                relevant_chunks.append(chunk)
        
        if not relevant_chunks:
            relevant_chunks = self.chunks[:top_k]
        
        return "\n\n---\n\n".join(relevant_chunks[:top_k])

# ==========================================
# åˆå§‹åŒ– Session State
# ==========================================
if 'conversations' not in st.session_state:
    st.session_state.conversations = {
        "å°è©± 1": {
            "messages": [],
            "memory_manager": VectorMemoryManager(),
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M")
        }
    }

if 'current_conversation' not in st.session_state:
    st.session_state.current_conversation = "å°è©± 1"

if 'conversation_counter' not in st.session_state:
    st.session_state.conversation_counter = 1

# ç²å–ç•¶å‰å°è©±
current_conv = st.session_state.conversations[st.session_state.current_conversation]
memory_manager = current_conv["memory_manager"]
messages = current_conv["messages"]

# ==========================================
# ğŸ“‚ å·¦å´é‚Šæ¬„ï¼šå°è©±ç®¡ç† + æ–‡ä»¶ä¸Šå‚³
# ==========================================
with st.sidebar:
    st.title("ğŸ¤– AI Agent åŠ©æ‰‹")
    
    # é¡¯ç¤ºç³»çµ±ç‹€æ…‹
    if VECTOR_AVAILABLE:
        st.success("âœ… å‘é‡æª¢ç´¢ç³»çµ±å·²å•Ÿç”¨")
    else:
        st.warning("âš ï¸ ä½¿ç”¨ç°¡åŒ–ç‰ˆæª¢ç´¢")
    
    st.divider()
    
    # æ–°å¢å°è©±æŒ‰éˆ•
    if st.button("â• æ–°å¢å°è©±", use_container_width=True):
        st.session_state.conversation_counter += 1
        new_conv_name = f"å°è©± {st.session_state.conversation_counter}"
        st.session_state.conversations[new_conv_name] = {
            "messages": [],
            "memory_manager": VectorMemoryManager(),
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M")
        }
        st.session_state.current_conversation = new_conv_name
        st.rerun()
    
    st.divider()
    
    # å°è©±åˆ—è¡¨
    st.subheader("ğŸ’¬ å°è©±è¨˜éŒ„")
    for conv_name in st.session_state.conversations.keys():
        conv_data = st.session_state.conversations[conv_name]
        msg_count = len(conv_data["messages"])
        
        col1, col2 = st.columns([4, 1])
        
        with col1:
            if st.button(
                f"{'ğŸ“Œ' if conv_name == st.session_state.current_conversation else 'ğŸ’­'} {conv_name} ({msg_count})",
                key=f"conv_{conv_name}",
                use_container_width=True
            ):
                st.session_state.current_conversation = conv_name
                st.rerun()
        
        with col2:
            if st.button("ğŸ—‘ï¸", key=f"del_{conv_name}", use_container_width=True):
                if len(st.session_state.conversations) > 1:
                    del st.session_state.conversations[conv_name]
                    st.session_state.current_conversation = list(st.session_state.conversations.keys())[0]
                    st.rerun()
                else:
                    st.warning("è‡³å°‘éœ€è¦ä¿ç•™ä¸€å€‹å°è©±")
    
    st.divider()
    
    # æ–‡ä»¶ä¸Šå‚³å€
    st.subheader("ğŸ“‚ æ–‡ä»¶ç®¡ç†")
    uploaded_file = st.file_uploader("ä¸Šå‚³ PDF æ–‡ä»¶", type=["pdf"], key="pdf_uploader")
    
    if uploaded_file:
        if not memory_manager.chunks:
            try:
                with st.spinner("ğŸ”„ æ­£åœ¨è™•ç† PDFï¼ˆæ­¥é©Ÿ 1-6ï¼‰..."):
                    reader = pypdf.PdfReader(uploaded_file)
                    chunks_count = memory_manager.load_pdf(reader)
                    
                    if VECTOR_AVAILABLE and len(memory_manager.embeddings) > 0:
                        st.success(f"âœ… å·²è¼‰å…¥ {chunks_count} å€‹å€å¡Šä¸¦å®Œæˆå‘é‡åŒ–")
                    else:
                        st.success(f"âœ… å·²è¼‰å…¥ {chunks_count} å€‹å€å¡Šï¼ˆä½¿ç”¨ç°¡åŒ–ç‰ˆæª¢ç´¢ï¼‰")
            except Exception as e:
                st.error(f"âŒ è®€å–å¤±æ•—ï¼š{e}")
        else:
            st.info(f"ğŸ“„ å·²è¼‰å…¥ï¼š{uploaded_file.name}")
    
    # è¨˜æ†¶é«”ç‹€æ…‹
    with st.expander("ğŸ§  VectorStore ç‹€æ…‹"):
        col_m1, col_m2 = st.columns(2)
        col_m1.metric("æ–‡æœ¬å€å¡Š", len(memory_manager.chunks))
        col_m2.metric("å‘é‡æ•¸é‡", len(memory_manager.embeddings) if hasattr(memory_manager, 'embeddings') else 0)
    
    st.divider()
    
    # ç³»çµ±è¨­å®š
    st.subheader("âš™ï¸ ç³»çµ±è¨­å®š")
    
    # âœ… API Key é è¨­å¡«å…¥
    gemini_api_key = st.text_input(
        "Google API Key", 
        value="",
        type="password"
    )
    
    # âœ… é è¨­ä½¿ç”¨ gemma-3-27b-it
    model_options = {
        "gemma-3-27b-it": "ğŸ¯ Gemma 3-27B (ä¸»è¦æ¸¬è©¦)",
        # "gemini-2.5-flash": "âš¡ Gemini 2.5 Flash (å‚™ç”¨)",
        # "gemini-3-flash": "âœ¨ Gemini 3 Flash (å‚™ç”¨)"
    }
    
    model_name = st.selectbox(
        "é¸æ“‡æ¨¡å‹",
        options=list(model_options.keys()),
        format_func=lambda x: model_options[x],
        index=0
    )
    
    st.caption("ğŸ’¡ å…¶ä»–å¯ç”¨æ¨¡å‹ï¼šgemini-2.5-flash, gemini-3-flash")
    
    # æª¢ç´¢åƒæ•¸è¨­å®š
    top_k = st.slider("æª¢ç´¢å€å¡Šæ•¸é‡ (top_k)", 1, 5, 3)
    
    st.divider()
    
    if st.button("ğŸ—‘ï¸ æ¸…ç©ºç•¶å‰å°è©±", use_container_width=True):
        current_conv["messages"] = []
        st.rerun()

# ==========================================
# ğŸ–¥ï¸ ä¸»è¦èŠå¤©å€åŸŸ
# ==========================================

st.title(f"ğŸ’¬ {st.session_state.current_conversation}")
st.caption(f"å»ºç«‹æ™‚é–“ï¼š{current_conv['created_at']} | è¨Šæ¯æ•¸ï¼š{len(messages)} | æ¨¡å‹ï¼š{model_name}")

chat_container = st.container()

with chat_container:
    if not messages:
        st.info("ğŸ‘‹ æ­¡è¿ä½¿ç”¨ AI Agent è«–æ–‡åŠ©æ‰‹ï¼è«‹ä¸Šå‚³ PDF æ–‡ä»¶ä¸¦é–‹å§‹æå•ã€‚")
        
        st.markdown("**ğŸ’¡ å¿«é€Ÿé–‹å§‹ç¯„ä¾‹ï¼š**")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ“Š çµ±è¨ˆé—œéµå­—", use_container_width=True):
                st.session_state.quick_prompt = "è«‹å¹«æˆ‘çµ±è¨ˆé€™ä»½æ–‡ä»¶ä¸­å‡ºç¾æœ€å¤šçš„é—œéµå­—"
        
        with col2:
            if st.button("ğŸ“ æ‘˜è¦æ–‡ä»¶", use_container_width=True):
                st.session_state.quick_prompt = "è«‹å¹«æˆ‘æ‘˜è¦é€™ä»½æ–‡ä»¶çš„ä¸»è¦å…§å®¹"
        
        with col3:
            if st.button("ğŸ” æ·±å…¥åˆ†æ", use_container_width=True):
                st.session_state.quick_prompt = "è«‹åˆ†æé€™ä»½æ–‡ä»¶çš„ç ”ç©¶æ–¹æ³•å’Œçµè«–"
    
    # é¡¯ç¤ºå°è©±æ­·å²
    # âœ… ä¿®æ­£ï¼šä½¿ç”¨ enumerate å–å¾—ç´¢å¼• iï¼Œä¸¦åŠ å…¥ key ä»¥é¿å… ID é‡è¤‡éŒ¯èª¤
    for i, msg in enumerate(messages):
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            
            if "thought_process" in msg and msg["thought_process"]:
                with st.expander("ğŸ” æŸ¥çœ‹ Agent åŸ·è¡Œéç¨‹"):
                    tab1, tab2, tab3 = st.tabs(["ğŸ“‹ åŸ·è¡Œè¨ˆç•«", "âš™ï¸ åŸ·è¡Œæ—¥èªŒ", "ğŸ“Š æª¢ç´¢å…§å®¹"])
                    
                    with tab1:
                        st.json(msg["thought_process"])
                    
                    with tab2:
                        if "execution_log" in msg:
                            st.code(msg["execution_log"], language="text")
                    
                    with tab3:
                        if "retrieved_content" in msg:
                            # âœ… é€™è£¡åŠ å…¥äº† key=f"retrieved_{i}"
                            st.text_area("æª¢ç´¢åˆ°çš„ç›¸é—œå…§å®¹", msg["retrieved_content"], height=200, key=f"retrieved_{i}")

# ==========================================
# ğŸ’¬ åº•éƒ¨è¼¸å…¥å€
# ==========================================

if hasattr(st.session_state, 'quick_prompt'):
    prompt = st.session_state.quick_prompt
    delattr(st.session_state, 'quick_prompt')
else:
    prompt = st.chat_input("ğŸ’­ è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ...", key="chat_input")

# ==========================================
# ğŸ¤– è™•ç†ä½¿ç”¨è€…è¼¸å…¥ï¼ˆå®Œæ•´ LangChain æµç¨‹ï¼‰
# ==========================================
if prompt:
    if not gemini_api_key:
        st.error("âŒ è«‹å…ˆåœ¨å·¦å´é‚Šæ¬„è¼¸å…¥ Google API Keyï¼")
        st.stop()
    
    messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        status_placeholder = st.empty()
        response_placeholder = st.empty()
        
        try:
            client = genai.Client(api_key=gemini_api_key)
            
            # å·¥å…·å¡å®šç¾©
            TOOL_CARDS = """
            ä½ æ“æœ‰ä»¥ä¸‹å·¥å…· (Tools) å¯ä»¥ä½¿ç”¨ï¼š
            1. [Vector_Retriever]
               - åŠŸèƒ½ï¼šä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦å¾çŸ¥è­˜åº«æª¢ç´¢æœ€ç›¸é—œçš„å…§å®¹
               - è¼¸å…¥ï¼šæœå°‹å•é¡Œæˆ–é—œéµå­— (String)
               - æ•ˆæœï¼šå°‡æª¢ç´¢åˆ°çš„æ–‡æœ¬å­˜å…¥ context['pdf_content']
               - æŠ€è¡“ï¼šEmbedding + Cosine Similarity
            
            2. [Python_Interpreter]
               - åŠŸèƒ½ï¼šåŸ·è¡Œ Python ç¨‹å¼ç¢¼é€²è¡Œè¨ˆç®—ã€çµ±è¨ˆæˆ–è³‡æ–™è™•ç†
               - è¼¸å…¥ï¼šPython ç¨‹å¼ç¢¼ (String)
               - å¯ç”¨è®Šæ•¸ï¼šcontext['pdf_content']
               - å¯ç”¨å‡½å¼ï¼šlen, sum, max, min, sorted, print, str, int, float ç­‰
            """
            
            recent_messages = messages[-6:] if len(messages) > 6 else messages
            history_str = "\n".join([f"{m['role']}: {m['content'][:200]}..." if len(m['content']) > 200 else f"{m['role']}: {m['content']}" for m in recent_messages])
            
            # Step 1: è¦åŠƒ
            status_placeholder.info("ğŸ¤” æ­£åœ¨åˆ†æå•é¡Œä¸¦åˆ¶å®šè¨ˆç•«...")
            
            planner_prompt = f"""
            ä½ æ˜¯ä¸€å€‹æ™ºèƒ½ Agent Plannerã€‚è«‹æ ¹æ“šä½¿ç”¨è€…å•é¡Œåˆ¶å®šåŸ·è¡Œè¨ˆç•«ã€‚
            {TOOL_CARDS}
            
            ã€å°è©±æ­·å²ã€‘ï¼š{history_str}
            ã€ç•¶å‰å•é¡Œã€‘ï¼š"{prompt}"
            
            è«‹è¼¸å‡º JSON è¨ˆç•«ï¼ˆä¸è¦åŒ…å« Markdown æ¨™è¨˜ï¼‰ï¼š
            {{
                "intent": "ä½¿ç”¨è€…æ„åœ–æè¿°",
                "reasoning": "é¸æ“‡é€™äº›æ­¥é©Ÿçš„åŸå› ",
                "steps": [
                    {{"tool": "Vector_Retriever", "args": "æœå°‹å•é¡Œ"}},
                    {{"tool": "Python_Interpreter", "args": "Pythonç¨‹å¼ç¢¼"}}
                ]
            }}
            """
            
            plan_resp = client.models.generate_content(model=model_name, contents=planner_prompt)
            raw_plan = clean_json_string(plan_resp.text)
            
            try:
                plan_data = json.loads(raw_plan) if raw_plan else {}
            except json.JSONDecodeError:
                plan_data = {"intent": "ç›´æ¥å›ç­”", "reasoning": "ç„¡æ³•è§£æè¨ˆç•«", "steps": []}
            
            # Step 2: åŸ·è¡Œå·¥å…·ï¼ˆæ­¥é©Ÿ 8-11ï¼‰
            status_placeholder.info("âš™ï¸ æ­£åœ¨åŸ·è¡Œå‘é‡æª¢ç´¢...")
            
            execution_logs = []
            context_data = {
                "pdf_content": "",
                "history": history_str,
                "user_query": prompt
            }
            
            retrieved_content = ""
            
            for i, step in enumerate(plan_data.get("steps", [])):
                tool = step.get("tool")
                args = step.get("args")
                
                if tool == "Vector_Retriever":
                    try:
                        # æ­¥é©Ÿ 8-11: Query â†’ Embedding â†’ Vector Similarity â†’ Related Chunks
                        res = memory_manager.retrieve(args, top_k=top_k)
                        context_data["pdf_content"] = res
                        retrieved_content = res
                        execution_logs.append(f"ã€å‘é‡æª¢ç´¢ã€‘å•é¡Œ: {args}\næª¢ç´¢åˆ° {len(res)} å­—ç¬¦çš„ç›¸é—œå…§å®¹")
                    except Exception as e:
                        execution_logs.append(f"ã€å‘é‡æª¢ç´¢éŒ¯èª¤ã€‘{e}")
                
                elif tool == "Python_Interpreter":
                    try:
                        old_stdout = sys.stdout
                        redirected_output = io.StringIO()
                        sys.stdout = redirected_output
                        
                        safe_builtins = {
                            "__builtins__": {
                                "len": len, "sum": sum, "max": max, "min": min,
                                "sorted": sorted, "print": print, "str": str,
                                "int": int, "float": float, "list": list,
                                "dict": dict, "set": set, "tuple": tuple,
                                "range": range, "enumerate": enumerate,
                                "zip": zip, "map": map, "filter": filter
                            }
                        }
                        
                        local_scope = {"context": context_data}
                        exec(args, safe_builtins, local_scope)
                        
                        sys.stdout = old_stdout
                        output = redirected_output.getvalue()
                        
                        if output.strip():
                            execution_logs.append(f"ã€PythonåŸ·è¡Œã€‘\n{output}")
                        else:
                            execution_logs.append("ã€PythonåŸ·è¡Œã€‘ç¨‹å¼åŸ·è¡Œå®Œæˆ")
                    
                    except Exception as e:
                        sys.stdout = old_stdout
                        execution_logs.append(f"ã€PythonéŒ¯èª¤ã€‘{e}")
            
            # âœ… ä¿®æ­£ï¼šå¦‚æœæ²’æœ‰æª¢ç´¢åˆ°å…§å®¹ï¼Œé è¨­æª¢ç´¢ä¸€æ¬¡ï¼ˆä¿åº•æ–¹æ¡ˆï¼‰
            if not context_data["pdf_content"] and memory_manager.chunks:
                try:
                    context_data["pdf_content"] = memory_manager.retrieve(prompt, top_k=top_k)
                    retrieved_content = context_data["pdf_content"]
                    execution_logs.append(f"ã€è‡ªå‹•æª¢ç´¢ã€‘ä½¿ç”¨å•é¡Œæœ¬èº«é€²è¡Œæª¢ç´¢ï¼Œæª¢ç´¢åˆ° {len(retrieved_content)} å­—ç¬¦")
                except Exception as e:
                    execution_logs.append(f"ã€è‡ªå‹•æª¢ç´¢å¤±æ•—ã€‘{e}")
            
            # Step 3: ç”Ÿæˆå›ç­”ï¼ˆæ­¥é©Ÿ 13-15ï¼‰
            status_placeholder.info("âœï¸ æ­£åœ¨ç”Ÿæˆå›ç­”...")
            
            final_context_log = "\n".join(execution_logs)
            
            # æ­¥é©Ÿ 13: Prompt Template
            final_prompt = f"""
            è«‹æ ¹æ“šä»¥ä¸‹è³‡è¨Šå›ç­”ä½¿ç”¨è€…å•é¡Œï¼š
            
            ã€ä½¿ç”¨è€…å•é¡Œã€‘ï¼š{prompt}
            ã€åŸ·è¡Œè¨ˆç•«ã€‘ï¼š{plan_data.get('intent', 'æœªçŸ¥')}
            ã€å·¥å…·åŸ·è¡Œçµæœã€‘ï¼š
            {final_context_log}
            
            ã€æª¢ç´¢åˆ°çš„ç›¸é—œå…§å®¹ã€‘ï¼ˆæ­¥é©Ÿ 12: Related Text Chunksï¼‰ï¼š
            {context_data['pdf_content'][:2000]}{"..." if len(context_data['pdf_content']) > 2000 else ""}
            
            è«‹æä¾›æ¸…æ™°ã€æœ‰æ¢ç†çš„å›ç­”ï¼Œä¸¦è§£é‡‹åŸ·è¡Œçµæœçš„æ„ç¾©ã€‚
            """
            
            # æ­¥é©Ÿ 14-15: LLM â†’ Answer
            final_resp = client.models.generate_content(model=model_name, contents=final_prompt)
            response_text = final_resp.text if final_resp.text else "(ç„¡å›æ‡‰)"
            
            status_placeholder.empty()
            response_placeholder.markdown(response_text)
            
            messages.append({
                "role": "assistant",
                "content": response_text,
                "thought_process": plan_data,
                "execution_log": final_context_log,
                "retrieved_content": retrieved_content
            })
            
            st.rerun()
        
        except Exception as e:
            status_placeholder.empty()
            response_placeholder.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            
            with st.expander("ğŸ”§ éŒ¯èª¤è©³æƒ…"):
                st.code(traceback.format_exc())
                st.write("**å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆï¼š**")
                st.write("1. æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢º")
                st.write("2. ç¢ºèªç¶²è·¯é€£ç·šæ­£å¸¸")

                st.write("3. å®‰è£å¿…è¦å¥—ä»¶ï¼špip install sentence-transformers")
